---
title: "Práctica 2"
author: "Oscar Ramirez González y Alejandro Prieto Velasco"
date: "26/5/2021"
output:
  html_document:
    highlight: default
    number_sections: yes
    theme: cosmo
    toc: yes
    toc_depth: 2
    includes:
      in_header: 75.584-PEC-header.html
  word_document: default
  pdf_document:
    highlight: zenburn
    toc: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Presentación

En esta práctica se elaborará un caso práctico orientado a
aprender a identificar los datos relevantes para un proyecto analítico y usar
las herramientas de integración, limpieza, validación y análisis de las mismas.

Para hacer esta práctica trabajamos en un grupo de 2
personas. Tenemos que entregar un solo archivo con el enlace Github
([https://github.com)](https://github.com)) donde se encuentren las soluciones incluyendo los nombres
de los componentes del equipo.

Se puede utilizar la Wiki de Github para describir nuestro
equipo y los diferentes archivos que corresponden a nuestra entrega. Cada
miembro del equipo tiene que contribuir con su usuario Github.

# Competencias

En esta práctica se desarrollan las siguientes competencias
del Master de Data Science:

Capacidad de analizar un problema en el nivel de
abstracción adecuado a cada situación y aplicar las habilidades y conocimientos
adquiridos para abordarlo y resolverlo.

Capacidad para aplicar las técnicas específicas
de tratamiento de datos (integración, transformación, limpieza y validación)
para su posterior análisis.

# Objetivos

Los objetivos concretos de esta práctica son:

Aprender a aplicar los conocimientos adquiridos
y su capacidad de resolución de problemas en entornos nuevos o poco conocidos
dentro de contextos más amplios o multidisciplinares.

Saber identificar los datos relevantes y los
tratamientos necesarios (integración, limpieza y validación) para llevar a cabo
un proyecto analítico.

Aprender a analizar los datos adecuadamente para
abordar la información contenida en los datos.

Identificar la mejor representación de los
resultados para aportar conclusiones sobre el problema planteado en el proceso
analítico.

Actuar con los principios éticos y legales
relacionados con la manipulación de dato en función del ámbito de aplicación.

Desarrollar las habilidades de aprendizaje que
les permitan continuar estudiando de un modo que tendrá que ser en gran medida
autodirigido o autónomo.

Desarrollar la capacidad de búsqueda, gestión y
uso de información y recursos en el ámbito de la ciencia de datos.

# Descripción del dataset.

Con el análisis del dataset del Titánic se quiere mostrar cómo
fue la distribución de la gente que murió y sobrevivió al hundimiento del barco
según diversos parámetros, realizando un análisis detallado de los datos disponibles
en el dataset como son el sexo, la clase contratada para realizar el viaje, si
viajaban con familia o no, etc.

El dataset original está compuesto por un total de 1309
pasajeros, y tiene 14 variables.

Para nuestro estudio hemos suprimido dos variables que
consideramos irrelevantes para el trabajo que queremos hacer sobre el dataset,
que son:

·       
Body: número de identificación del cuerpo

·       
Home.dest: lugar de origen y de destino del
pasajero

Se ha dividido el dataset en dos conjuntos, uno de train con
891 pasajeros, y otro de test con 418 pasajeros, para poder hacer una
predicción con el modelo creado sobre si cada uno de los 418 pasajeros habría
sobrevivido o no siguiendo el modelo creado.

A continuación, se da una descripción detallada de cada una
de las 12 variables utilizadas en el dataset:

1.     
PassengerId: número único de orden para
identificar a cada uno de los pasajeros.

2.     
Survived: 0 = no sobrevivio, 1 = si sobrevivio

3.     
Pclass: clase en la que realizaba el viaje cada
pasajero.

·       
1: primera clase

·       
2: segunda clase

·       
3: tercera clase

4.     
Name: nombre completo de cada pasajero

5.     
Sex: sexo de cada pasajero

6.     
Age: edad de cada pasajero

7.     
SibSp: número de familiares a bordo de cada
pasajero de estos tipos: hermano, hermana, cuñado, cuñada, pareja. Prometidos y
amantes no incluidos

8.     
Parch: número de familiares a bordo de cada
pasajero de estos tipos: madre, padre, hijo, hija, hijastro, hijastra.

9.     
Ticket: número de billete

10.  
Fare: tárifa del billete en libras inglesas

11.  
Cabin: número de camarote

12.  
Embarked: lugar de embarque

·       
C=Cherbourg;

·       
Q = Queenstown

·       
S = Southampton

Este dataset ha sido creado por varios investigadores, entre
ellos:

·       
Eaton & Haas (1994) Titanic: Triumph and
Tragedy. Fueron uno de los creadores originales.

·       
Patrick Stephens Ltd, incluye una lista de
pasajeros creado por muchos investigadores y editada por Michael A. Findlay.

Con el análisis detallado de este dataset se persigue saber
de una manera lo más objetiva posible si las circunstancias de cada pasajero
(sexo, edad, si iba con familiares, la clase en la que viajaba, etc) influyó de
alguna manera en la supervivencia o no de los pasajeros. Además, se incluye un
modelo de predicción que nos permite estimar si un pasajero habría sobrevivido
o no al hundimiento del Titanic en función del análisis realizado y de los
datos de dicho pasajero.

# Librería

```{r message=FALSE, warning=FALSE}

if (!require(ggplot2)) {
  install.packages('ggplot2')
  library(ggplot2)
}

if (!require(plyr)) {
  install.packages("plyr")
  library(plyr)
}


if (!require(dplyr)) {
  install.packages("dplyr")
  library(dplyr)
}

if (!require(gridExtra)) {
  install.packages("gridExtra")
  library(gridExtra)
}


```

# Integración y análisis inicial

El dataset viene ya dividido en train y test, en los datos set no está la variable objetivo "Survived" la cual es la que hay predecir para la competición. Vamos a unir ambos para poder trabajarlos de forma más sencilla, una vez los datos hayan sido limpiados y tratados volveremos a separarlos.

```{r message=FALSE, warning=FALSE}

# Loading datest
train <- read.csv("train.csv")
test <- read.csv("test.csv")

# Combine dataset
full_original <- bind_rows(train, test)

# Create a copy to work with

full <- full_original
```

Ahora vamos a estudiar un poco el contenido del dataset, primero vamos a ver las 10 primeras filas y un primer summary.

```{r message=FALSE, warning=FALSE}
head(full, 10)

# First summary of the info
summary(full)
```

Vemos como el set contiene un total de 1309 registros de los cuales 418 pertenecen al set test (son los NA's de la columna "Survived") y el resto a al set de entrenamiento.

Observamos como la variable "Survived" toma valores de 1 (sobrevive) y 0 (muere), vamos a cambiar la nomenclatura para que se más fácil de entender.. Vemos como la variable Pclass está como integer, quizá interese cambiarla a factor ya que en realidad esta es una variable categórica, el igual que Sex, Ticket y Embark, aunque estas las ha leído como caracteres.

Ahora vamos cambiar el formato de las columnas.

```{r message=FALSE, warning=FALSE}
# Renaming 0 and 1 to "Diddn't survived" and "Survived"
full$Survived[full$Survived == 0] <- "Didn't survived"
full$Survived[full$Survived == 1] <- "Survived"

# Changing to factor
full$Survived <- as.factor(full$Survived)
full$Pclass <- as.factor(full$Pclass)
full$Sex <- as.factor(full$Sex)
full$Embarked <- as.factor(full$Embarked)
```

# Limpieza de los datos.

En esta etapa del proyecto vamos primero a identificar y tratar aquellos datos que estén vacíos, y después vamos a pasar a tratar los outliers.

## Datos vacíos

```{r message=FALSE, warning=FALSE}
# Let's see the NA by columns
colSums(is.na(full))
```

Vemos que hay 263 NA's en Age y 1 en Fare. Vamos a ver las distribuciones de estas variables para así poder entender mejor como se distribuyen estas dos variables.

```{r message=FALSE, warning=FALSE}
# Plotting histogram of Age
ggplot(full, aes(x = Age)) + geom_density()

# Plotting histogram of Fare
ggplot(full, aes(x = Fare)) + geom_density()
```

Vemos que tanto Age como Fare tienen una asimetría positiva esto junto al hecho de que el dataset es pequeño y no vamos a considerar borrar aquellas lineas con NA, nos hace tomar la decisión de imputar estos NA por la mediana

```{r message=FALSE, warning=FALSE}
# Filling NA in Age with miss forest
full$Age[is.na(full$Age)] <- median(full$Age, na.rm = TRUE)

# Filling Fare NA with median
full$Fare[is.na(full$Fare)] <- median(full$Fare, na.rm = TRUE)
```

También hay un serie de NA que a pesar de no haber aparecido inicialmente están ahí. Vamos a verlos.

```{r message=FALSE, warning=FALSE}
colSums(full == "")
```

Parece ser que en cabin la gran mayoría de valores están vacíos, vamos a sustituir estos por Unknown.

```{r message=FALSE, warning=FALSE}
# Replacing epmpty values for Unknown
full$Cabin[full$Cabin == ""] <- "Unknown"
```

Al ser solo 2 vamos a sustituir los valores vacíos de embarked por la clase mayoritaria

```{r message=FALSE, warning=FALSE}
# Group by and count by embarked
full %>% 
  group_by %>%
  count(Embarked)

# Replace empties for S
full$Embarked[full$Embarked == ""] <- "S"
```

## Tratamiento de outliers

Vamos a identificar los posibles outliers en la variables numéricas mediante boxplots.

```{r message=FALSE, warning=FALSE}

# Making boxplots
box_age <- 
  ggplot(full, aes(x = Age)) + geom_boxplot(colour = "red")
box_SibSp <-
  ggplot(full, aes(x = SibSp)) + geom_boxplot(colour = "cyan")
box_Parch <-
  ggplot(full, aes(x = Parch)) + geom_boxplot(colour = "blue")
box_Fare <-
  ggplot(full, aes(x = Fare)) + geom_boxplot(colour = "brown")


# Grouping them in a grid
grid.arrange(box_age, box_SibSp, box_Parch, box_Fare)

```

------------------------------------------------------------------------

Vemos como hay posibles outliers en todas las variables, vamos a ver los histogramas, para identificar posibles grupos.

En la variable Age todos parecen están en un rango lógico de 0 a 80 años.

Con respecto a SipSp (pariente o esposas a bordo) y Parch (Parents/ children), parece lógico que alguien pueda sumar hasta 8 ente parientes y la esposa o entre hijos y padres (si tenemos en cuenta la época no es d extrañar familias con 8 hijos).

Vamos a ver en más detalle la variable Fare, parece que hay un valor muy alto en torno a 500. Vamos a dividir el boxplot por clase y ver como se distribuye.

```{r message=FALSE, warning=FALSE}

ggplot(full, aes(x = Fare, y = Pclass, fill = Pclass)) + geom_boxplot( )
```

Vemos como ahora aunque haya algunos puntos fuera de los bigotes en 2 y 3 clase, no están tan poco muy alejados. El punto de 500 en primera clase podría ser el precio de la suite más lujosa del barco, así que por ahora lo vamos a dejar.

## Creación de nuevas variables

Este data set presenta varias variables interesantes que, o bien combinándolas con otras, o bien mediante la extracción de información de ellas.

La primera nueva variable que vamos a crear va a ser a partir de Name. Esta variable representa el nombre apelli y título de cada pasajero (Miss, Mr., etc.), vamos a extraerlos primero y despues consolidarlos para tener solo los títulos "Other", "Mr", "Mr " y "Miss".

```{r message=FALSE, warning=FALSE}
# Extracting the title form the Name column
full$Title <- as.factor(gsub('(.*, )|(\\..*)', '', full$Name))

# Showing different levels of the factor
levels(full$Title)

# Consolidating names
full$Title <-
  revalue(
    full$Title,
    c(
      "Capt" = "Other",
      "Col" = "Other",
      "Don" = "Mr",
      "Dona" = "Mrs",
      "Dr" = "Other",
      "Jonkheer" = "Other",
      "Lady" = "Miss",
      "Major" = "Other",
      "Master" = "Mr",
      "Mlle" = "Miss",
      "Mme" = "Mrs",
      "Ms" = "Miss",
      "Rev" = "Other",
      "Sir" = "Other",
      "the Countess" = "Other"
    )
  )

# Cheking they make sense with the sex
table(full$Sex, full$Title)
```

La siguiente variable que vamos a crear es el tamaño de la familia con las que viaja cada pasajero a partir de las variable SibSp y Parch, además de sumarle 1 del propio pasajero

```{r message=FALSE, warning=FALSE}
# Family size, from SibSp and Parch + the passenger itself
full$FamilyS <- full$SibSp + full$Parch + 1
```

!!!!!!!!!!!!corregir unknown!!!!!!!!!!!!!!!!!!!!!!!!!

También podemos extraer la planta en la que se hospeda cada pasajero sabiendo el número de habitación que tienen. Esto podría influir en un modelo que prediga la supervivencia.

```{r message=FALSE, warning=FALSE}
#Let's extract the deck from the cabin column
full$Deck <- as.factor(substr(full$Cabin, 0, 1))
```

oscainicio
# Análisis estadístico

En este apartado vamos a relizar un análisis estadístico de los datos numéricos
del dataset del Titanic.

## Análisis de la normalidad

En primer lugar vamos a realizar un histograma de las variables númericas para 
intentar dilucidar de manera gráfica si tenemos alguna variable con una 
distribución normal.
```{r message=FALSE, warning=FALSE}
print("COMIENZO DEL ANALISIS DE LA NORMALIDAD")
dim(full)
hist(full$Age)
hist(full$Fare)
hist(train$Survived)
hist(train$Pclass)
hist(full$FamilyS)

```

De las gráficas obtenidas, se puede observar que solamente la variable Age  
podría tener una distribución cercana a la normal, pero hace falta verificarlo.

Vamos a verificar que es así (o no) mediante la prueba de normalidad de 
Anderson-Darling.

Lo que hacemos es verificar que para cada variable se obtiene un valor mayor
al nivel de significación elegido, en esta caso alfa = 0.02. Si esto ocurre, 
tenemos que la variable sigue una distribución normal.


```{r message=FALSE, warning=FALSE}

library(nortest)
alpha = 0.05
col.names = colnames(full)
for (i in 1:ncol(full)) {
  if (i == 1) cat("Variables que no siguen una distribución normal:\n\n")
  if (is.integer(full[,i]) | is.numeric(full[,i])) {
    p_val = ad.test(full[,i])$p.value
    if (p_val < alpha) {
      cat("La variable",col.names[i], "no tiene una distribución normal", "\n")
    }
    else {
      print("lucas")
    }
  }
}

```

No tenemos ninguna de las variables de nuestro dataset que siga una distribución 
normal, de acuerdo con el criterio de Anderson-Darling. 

Para hacer una segunda comprobación, vamos a utiliar el test de Shapiro-Wilk para
la edad:

```{r message=FALSE, warning=FALSE}
print(shapiro.test(full$Age))

```
De nuevo vemos que el p-values es mucho más pequeño que el valor mínimo para
considerar que tenemos una variable con una distribución normal.


## Análisis de la homogeneidad de la varianza (homocedasticidad)

Con el suspuesto de homogeneidad de la varianza se persigue comprobar si dos
variables distintas de un mismo conjunto de datos tienen una varianza constante.

Como recordatorio ponemos debajo que es la varianza y los elementos con los que 
se calcula.

![Varianza](./Varianza.jpg "Varianza")

![Valores para el cálculo de la varianza](./ElementosVarianza.jpg "Elementos Varianza")
Con las distribuciones que tenemos en cada una de las variables va a ser muy 
complicado, por no decir imposible, encontrar dos de estas variables con las 
que se pueda hacer un análisis de la homogeneidad de la varianza. Al final 
lo que queremos comprobar es que la varizanza de las dos variables elegidas

Se podrían utilizar los siguientes tests:

- F-test (razón de varianzas): se recomienda para dos variables con una
clara distribución normal. No tenemos el escenario más adecuado para aplicar
este test.

- Test de Levene. Este método permite comparar dos o más variables sin que 
necesariamente tengan una distribución normal. Vamos a comparar las variables
Edad y Fare.


- Test de Bartlett: permite contrastar la igualdad de varizand en 2 o más 
poblaciones sin necesidad de que el tamañon de los grupos sea el mismo. Es
la mejor opción si se esta seguro de que los datos provienen de una
distribución normal.

- Test de Brown-Forsyth: es equivalente al test de Levene cuando se emplea
la mediana como medida de centralidad.

- Test de Fligner-Killeen: es un test no paramétrico que compara las varianzas
basándose en la mediana. Es tambien una alternativa cuando no se cumple la
condición de normalidad en las muestras.

Con las premisas indicadas anteriormente, vamos a usar el test de 
Fligner-Killeen para la combinación de varias parejas de variables..

```{r message=FALSE, warning=FALSE}


a <- full[full$Sex == "female","Age"]
b <- full[full$Sex == "male","Age"]

fligner.test(x = list(a,b))


a <- full[full$Survived == "Didn't survived","Age"]
b <- full[full$Survived == "Survived","Age"]

fligner.test(x = list(a,b))


a <- full[full$Pclass == "1","Age"]
b <- full[full$Pclass == "2","Age"]
b <- full[full$Pclass == "3","Age"]

fligner.test(x = list(a,b))

```
Como se puede ver en los resultados obtenidos, solamente en el caso de la
relación entre sexo y edad tenemos un p-value que se acerca a 0.05, que se
considera el límite para considerar que tenemos homocedasticidad en las
dos variables analizadas. En ninguna de las tres relaciones estudiadas tenemos
homocedasticidad. Podemos decir que las variables cumplen el criterio de 
heterocedasticidad.


## Pruebas estadísticas
En primer lugar estudiamos la correlación entre las variables numéricas
presentes en el dataset.

```{r message=FALSE, warning=FALSE}

full$Pclass = as.numeric(as.character(full$Pclass))

print(full)

num<-full[,c(3,6,7,8,10)]

colnames(num)<-c("Clase_Pasajero","Edad","Pariente_Masculino","Pariente_Femenino","Tarifa_Pasaje")
covar<-round(var(num),3)
print("covar")
covar

corr<-round(cor(num),2)
print("correlc")
corr

```
Podemos ver como en al caso de la correlación, a mayor edad la clase del pasajero 
tambien es más alta, considerando una clase más alta un valor más bajo de dicho
parametro (1 es primer clase), por lo que la correlación nos sale negativa.

Lo mismo ocurre con la tarifa del pasaje y la clase del pasaje: a mayor clase
(menor número) tenemos una tarifa mayor.

oscarfinal

# Visualizaciones

```{r message=FALSE, warning=FALSE}
#Plots

Class_surv_plot <-
  ggplot(train, aes(x = Pclass, fill = as.factor(Survived))) + geom_bar(show.legend = FALSE)

Sex_surv_plot <-
  ggplot(train, aes(x = Sex, fill = as.factor(Survived))) + geom_bar(show.legend = FALSE)

Age_surv_plot <-
  ggplot(train, aes(x = Age, fill = as.factor(Survived))) + geom_histogram(show.legend = FALSE)
Fare_plot <-
  ggplot(train, aes(
    x = as.factor(Pclass),
    y = Fare,
    fill = as.factor(Pclass)
  )) + geom_boxplot(show.legend = FALSE)


grid.arrange(Class_surv_plot, Sex_surv_plot, Age_surv_plot, Fare_plot)
```

# Conclusiones
